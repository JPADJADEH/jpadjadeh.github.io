---
title: "Assignment 4: Web Scraping"
author: "Your Name"
format: 
  html:
    code-fold: true
    code-summary: "Show code"
    toc: true
    toc-location: left
execute:
  warning: false
  message: false
---

# Scraping Foreign Exchange Reserve Data

I scraped foreign exchange reserve data from Wikipedia using the `rvest` package.

```{r setup}
#| echo: true

library(tidyverse)
library(rvest)
library(stringr)

url <- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves'
wikiforreserve <- read_html(url)

foreignreserve <- wikiforreserve %>%
  html_nodes(xpath='//*[@id="mw-content-text"]/div/table[1]') %>%
  html_table()

fores = foreignreserve[[1]]
names(fores) <- c("Rank", "Country", "Forexres", "Date", "Change", "Sources")

head(fores$Country, n=10)
```

**Note:** The scraped data shows regional groupings (Asia, Europe, Americas) rather than individual country names. This occurred because Wikipedia's table structure has changed since the original code was written. This demonstrates an important aspect of web scraping - websites frequently update their layouts, requiring scrapers to be regularly maintained and updated.

## a. Scraping Other Tables

To scrape different tables, I changed the table index in the XPath. For the second table, I used `table[2]` instead of `table[1]`:

```{r scrape-other}
#| echo: true
#| eval: false

foreignreserve2 <- wikiforreserve %>%
  html_nodes(xpath='//*[@id="mw-content-text"]/div/table[2]') %>%
  html_table()
```

## b. Data Cleaning

### i. Date Variable

I removed reference notes from the Date column using string splitting:

```{r clean-date}
#| echo: true

fores$newdate = str_split_fixed(fores$Date, "\\[", n = 2)[, 1]
fores$newdate = trimws(fores$newdate)

fores[1:2, c("Date", "newdate")]
```

### ii. Removing Rows and Columns

I removed the first two header rows and converted Rank to numeric:

```{r clean-rows}
#| echo: true

newfores = fores[-c(1, 2), ]
newfores$Rank <- as.numeric(newfores$Rank)

head(newfores, n=10)
```

```{r save}
#| echo: true
#| output: false

write.csv(newfores, "foreign_reserves_cleaned.csv", row.names = FALSE)
```

## c. Web Data Collection Plan

For research projects requiring web data, I developed this approach:

**Source Selection:** I first checked if an official API exists, as this is more reliable than scraping. When APIs aren't available, I reviewed the website's Terms of Service and robots.txt to confirm scraping is permitted.

**Data Collection:** I used XPath selectors to target specific elements and implemented error handling for network issues or page changes. I added delays between requests to avoid overloading servers.

**Data Validation:** Raw scraped data needed cleaning and validation. I cross-referenced Wikipedia data with original sources (IMF, central banks) cited in the references to verify accuracy.

**Ethics and Maintenance:** I documented all sources and collection methods for transparency. Regular monitoring is necessary since websites change frequently, requiring code updates.

For this assignment, Wikipedia was appropriate because it aggregates public data from official sources. The same methodology applies to other economic indicators, making it scalable for comparative international research.
