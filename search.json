[
  {
    "objectID": "Assignment1.html",
    "href": "Assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Anscombe’s Quartet shows that datasets can share nearly identical summary statistics (means, variances, correlations, and regression lines) yet have very different relationships when graphed. One series is roughly linear with noise, another is clearly non-linear (curved), another is driven by an outlier, and the last has vertical clustering forced by a high-leverage point.\nWhy this matters: Relying only on summary numbers can be misleading.\nBest practice: Always visualize the data (e.g., scatterplots) before and after modeling to catch:\n- Outliers / high-leverage points\n- Non-linear patterns\n- Heterogeneity or clusters\n\n# Load dataset\ndata(anscombe)\n\n# Build formulas and fit models\nff &lt;- y ~ x\nmods &lt;- setNames(vector(\"list\", 4), paste0(\"lm\", 1:4))\nfor (i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  mods[[i]] &lt;- lm(ff, data = anscombe)\n}\n\n# Panel layout and margins\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma = c(0, 0, 2, 0))\n\n# Plot 4 panels with regression lines\nfor (i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"blue\")\n}\n\nmtext(\"Anscombe's 4 Regression Data Sets\", outer = TRUE, cex = 1.3)\n\n\n\n\n\n\n\npar(op)"
  },
  {
    "objectID": "Assignment1.html#q1.-analyzing-anscombes-plots",
    "href": "Assignment1.html#q1.-analyzing-anscombes-plots",
    "title": "Assignment 1",
    "section": "",
    "text": "Anscombe’s Quartet shows that datasets can share nearly identical summary statistics (means, variances, correlations, and regression lines) yet have very different relationships when graphed. One series is roughly linear with noise, another is clearly non-linear (curved), another is driven by an outlier, and the last has vertical clustering forced by a high-leverage point.\nWhy this matters: Relying only on summary numbers can be misleading.\nBest practice: Always visualize the data (e.g., scatterplots) before and after modeling to catch:\n- Outliers / high-leverage points\n- Non-linear patterns\n- Heterogeneity or clusters\n\n# Load dataset\ndata(anscombe)\n\n# Build formulas and fit models\nff &lt;- y ~ x\nmods &lt;- setNames(vector(\"list\", 4), paste0(\"lm\", 1:4))\nfor (i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  mods[[i]] &lt;- lm(ff, data = anscombe)\n}\n\n# Panel layout and margins\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma = c(0, 0, 2, 0))\n\n# Plot 4 panels with regression lines\nfor (i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"blue\")\n}\n\nmtext(\"Anscombe's 4 Regression Data Sets\", outer = TRUE, cex = 1.3)\n\n\n\n\n\n\n\npar(op)"
  },
  {
    "objectID": "Assignment1.html#q.1-analyzing-anscombes-plots",
    "href": "Assignment1.html#q.1-analyzing-anscombes-plots",
    "title": "Assignment 1",
    "section": "Q.1 Analyzing Anscombe’s Plots",
    "text": "Q.1 Analyzing Anscombe’s Plots\nAnscombe’s Quartet teaches us that data can look the same in numbers but tell very different stories when you make a graph. All four datasets have the same averages, correlations, and regression lines, but the plots show one is a straight line, another is curved, and others are shaped by outliers.\nThe problem is that if we only look at numbers, we may miss these hidden patterns and make wrong conclusions.\nSolution: Always graph the data before and after running models. Simple plots like scatterplots can reveal issues such as:\n- Outliers\n- Non-linear trends\n- Unusual points"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Author",
    "section": "",
    "text": "John-Paul Adjadeh is a Ph.D. Candidate in Political Science and M.S. student in Social Data Analytics at the University of Texas at Dallas. His research explores traditional authority, democratic governance, and AI-informed policy, drawing on experience at Ghana’s Ministry of Chieftaincy and collaborations with the Governance and Local Development Institute."
  },
  {
    "objectID": "Data_Visualization/Assignments/Assignment1.html",
    "href": "Data_Visualization/Assignments/Assignment1.html",
    "title": "Assignment 1: Data Visualization",
    "section": "",
    "text": "Anscombe’s Quartet teaches us that data can look the same in numbers but tell very different stories when you make a graph. All four datasets have the same averages, correlations, and regression lines, but the plots show one is a straight line, another is curved, and others are shaped by outliers.\nThe problem: If we only look at numbers, we may miss these hidden patterns and make wrong conclusions.\nThe solution: Always graph the data before and after running models. Simple plots like scatterplots can reveal issues like outliers, non-linear trends, or unusual points.\n\n\n\n\n\n\n\n\n\n# Set up plotting layout\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma = c(0, 0, 2, 0))\n\n# Plot all four datasets with regression lines\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13),\n       main = paste(\"Dataset\", i))\n  abline(mods[[i]], col = \"blue\", lwd = 2)\n}\nmtext(\"Anscombe's 4 Regression Data Sets\", outer = TRUE, cex = 1.5)\n\n\n\n\nAnscombe’s Quartet: Four datasets with identical summary statistics but very different patterns\n\n\n\npar(op)\n\n\n\n\nThe visualization reveals four very different relationships:\n\nDataset 1: Perfect linear relationship\nDataset 2: Clear non-linear (curved) relationship\n\nDataset 3: Linear relationship with one outlier\nDataset 4: No relationship except for one influential point\n\nDespite these dramatic differences, all four datasets have: - Same mean for x and y - Same correlation coefficient - Same regression line - Same R-squared value\nThis demonstrates why visualization is essential for understanding data patterns that summary statistics alone cannot reveal."
  },
  {
    "objectID": "Data_Visualization/Assignments/Assignment1.html#analyzing-anscombes-plots",
    "href": "Data_Visualization/Assignments/Assignment1.html#analyzing-anscombes-plots",
    "title": "Assignment 1: Data Visualization",
    "section": "",
    "text": "Anscombe’s Quartet teaches us that data can look the same in numbers but tell very different stories when you make a graph. All four datasets have the same averages, correlations, and regression lines, but the plots show one is a straight line, another is curved, and others are shaped by outliers.\nThe problem: If we only look at numbers, we may miss these hidden patterns and make wrong conclusions.\nThe solution: Always graph the data before and after running models. Simple plots like scatterplots can reveal issues like outliers, non-linear trends, or unusual points.\n\n\n\n\n\n\n\n\n\n# Set up plotting layout\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma = c(0, 0, 2, 0))\n\n# Plot all four datasets with regression lines\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13),\n       main = paste(\"Dataset\", i))\n  abline(mods[[i]], col = \"blue\", lwd = 2)\n}\nmtext(\"Anscombe's 4 Regression Data Sets\", outer = TRUE, cex = 1.5)\n\n\n\n\nAnscombe’s Quartet: Four datasets with identical summary statistics but very different patterns\n\n\n\npar(op)\n\n\n\n\nThe visualization reveals four very different relationships:\n\nDataset 1: Perfect linear relationship\nDataset 2: Clear non-linear (curved) relationship\n\nDataset 3: Linear relationship with one outlier\nDataset 4: No relationship except for one influential point\n\nDespite these dramatic differences, all four datasets have: - Same mean for x and y - Same correlation coefficient - Same regression line - Same R-squared value\nThis demonstrates why visualization is essential for understanding data patterns that summary statistics alone cannot reveal."
  },
  {
    "objectID": "Data_Visualization/Assignments/Assignment1.html#changing-color-in-fall-script",
    "href": "Data_Visualization/Assignments/Assignment1.html#changing-color-in-fall-script",
    "title": "Assignment 1: Data Visualization",
    "section": "2. Changing Color in Fall Script",
    "text": "2. Changing Color in Fall Script\nThis section demonstrates generative art using L-systems to create plant-like structures. The original script uses “burlywood3” color, but we can experiment with different fall colors to create various artistic effects.\n\nSetup Required Packages\n\n\nGenerating the Fall Plant Art\n\n# Define elements in plant art\naxiom &lt;- \"X\"\nrules &lt;- list(\"X\"=\"F-[[X]+X]+F[+FX]-X\", \"F\"=\"FF\")\nangle &lt;- 22.5\ndepth &lt;- 6\n\n# Generate the L-system string\nfor (i in 1:depth) axiom &lt;- gsubfn(\".\", rules, axiom)\n\n# Extract actions\nactions &lt;- str_extract_all(axiom, \"\\\\d*\\\\+|\\\\d*\\\\-|F|L|R|\\\\[|\\\\]|\\\\|\") %&gt;% unlist\n\n# Initialize data structures\nstatus &lt;- data.frame(x=numeric(0), y=numeric(0), alfa=numeric(0))\npoints &lt;- data.frame(x1 = 0, y1 = 0, x2 = NA, y2 = NA, alfa=90, depth=1)\n\n# Generate drawing data\nfor (action in actions) {\n  if (action==\"F\") {\n    x &lt;- points[1, \"x1\"] + cos(points[1, \"alfa\"]*(pi/180))\n    y &lt;- points[1, \"y1\"] + sin(points[1, \"alfa\"]*(pi/180))\n    points[1,\"x2\"] &lt;- x\n    points[1,\"y2\"] &lt;- y\n    data.frame(x1 = x, y1 = y, x2 = NA, y2 = NA,\n               alfa=points[1, \"alfa\"],\n               depth=points[1,\"depth\"]) %&gt;% rbind(points) -&gt; points\n  }\n  if (action %in% c(\"+\", \"-\")){\n    alfa &lt;- points[1, \"alfa\"]\n    points[1, \"alfa\"] &lt;- eval(parse(text=paste0(\"alfa\",action, angle)))\n  }\n  if(action==\"[\"){\n    data.frame(x=points[1, \"x1\"], y=points[1, \"y1\"], alfa=points[1, \"alfa\"]) %&gt;%\n      rbind(status) -&gt; status\n    points[1, \"depth\"] &lt;- points[1, \"depth\"]+1\n  }\n  if(action==\"]\"){\n    depth &lt;- points[1, \"depth\"]\n    points[-1,] -&gt; points\n    data.frame(x1=status[1, \"x\"], y1=status[1, \"y\"], x2=NA, y2=NA,\n               alfa=status[1, \"alfa\"],\n               depth=depth-1) %&gt;%\n      rbind(points) -&gt; points\n    status[-1,] -&gt; status\n  }\n}\n\n# Create the plot with modified fall color\nggplot() +\n  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2),\n               lineend = \"round\",\n               color=\"darkred\", # Changed from original \"burlywood3\" to \"darkred\"\n               data=na.omit(points)) +\n  coord_fixed(ratio = 1) +\n  theme_void() +\n  labs(title = \"Fall Plant Art - L-System Generated\",\n       subtitle = \"Color changed from burlywood3 to darkred for autumn theme\")\n\n\n\n\nGenerative plant art using L-systems - Modified with different fall colors\n\n\n\n\n\n\nColor Experiment Notes\nThe original script used color=\"burlywood3\" which creates a tan/brown appearance. For this fall-themed version, I changed it to color=\"darkred\" to evoke autumn foliage. Other fall color options you could experiment with include:\n\n\"orange\" - Bright autumn orange\n\"goldenrod\" - Golden yellow fall color\n\n\"saddlebrown\" - Rich brown\n\"firebrick\" - Deep red\n\"chocolate\" - Warm brown\n\nThe L-system uses rules to recursively generate a tree-like structure, creating organic, plant-like patterns through mathematical iteration."
  },
  {
    "objectID": "Data_Visualization/Assignments/Assignment1.html#critique-of-the-plot-lessons-from-nathan-yau",
    "href": "Data_Visualization/Assignments/Assignment1.html#critique-of-the-plot-lessons-from-nathan-yau",
    "title": "Assignment 1",
    "section": "",
    "text": "This plot tries to show too many things at once—different contrasts, shapes, colors, and line styles—making it cluttered and hard to read.\nAs Nathan Yau emphasizes, a good visualization should highlight the story clearly, but here the message is buried under visual noise.\nProblems in the plot:\n- Long inline labels crowd the chart.\n- The zero line (a key reference) is not emphasized enough.\nCleaner design would:\n- Reduce the number of comparisons per plot.\n- Use consistent symbols.\n- Move labels to a legend.\n- Highlight the reference line.\nThis would make the main story—about winners, losers, and economic perceptions—much easier to see."
  },
  {
    "objectID": "Data_Visualization/Assignments/Assignment1.html#reflections",
    "href": "Data_Visualization/Assignments/Assignment1.html#reflections",
    "title": "Assignment 1",
    "section": "Reflections",
    "text": "Reflections\n\nAI is not just about replacing workers; it’s about empowering them with better tools.\nCollaborative AI means machines handle repetitive tasks while humans focus on higher-value work.\nMany panic about job loss, but the real challenge is learning AI’s rudiments and embracing its advantages."
  },
  {
    "objectID": "Data_Visualization/Assignments/Assignment1.html#key-lessons",
    "href": "Data_Visualization/Assignments/Assignment1.html#key-lessons",
    "title": "Assignment 1",
    "section": "Key Lessons",
    "text": "Key Lessons\n\nEmpowerment, not replacement.\n\nCollaboration between humans and machines.\n\nOvercoming fear of AI through adaptation.\n\nTrust and transparency for adoption."
  },
  {
    "objectID": "Data_Visualization/Assignments/Assignment1.html#conclusion",
    "href": "Data_Visualization/Assignments/Assignment1.html#conclusion",
    "title": "Assignment 1",
    "section": "Conclusion",
    "text": "Conclusion\nIndustrial AI represents transformation, not displacement.\nThe future of work is collaboration where both humans and machines thrive."
  },
  {
    "objectID": "Assignment1.html#q1.-analyzing-anscombes-plots-1",
    "href": "Assignment1.html#q1.-analyzing-anscombes-plots-1",
    "title": "Assignment 1",
    "section": "Q1. Analyzing Anscombe’s Plots",
    "text": "Q1. Analyzing Anscombe’s Plots\nAnscombe’s Quartet shows that datasets can share nearly identical summary statistics (means, variances, correlations, and regression lines) yet have very different relationships when graphed. One series is roughly linear with noise, another is clearly non-linear (curved), another is driven by an outlier, and the last has vertical clustering forced by a high-leverage point.\nWhy this matters: Relying only on summary numbers can be misleading.\nBest practice: Always visualize the data (e.g., scatterplots) before and after modeling to catch outliers/high-leverage points, non-linear patterns, and clusters.\n```{r setup, include=FALSE} knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)"
  },
  {
    "objectID": "Data_Visualization/Assignments/Assignment1.html#critique-of-plot-design-nathan-yau-principles",
    "href": "Data_Visualization/Assignments/Assignment1.html#critique-of-plot-design-nathan-yau-principles",
    "title": "Assignment 1: Data Visualization",
    "section": "3. Critique of Plot Design (Nathan Yau Principles)",
    "text": "3. Critique of Plot Design (Nathan Yau Principles)\nThe plot below demonstrates several visualization design issues that violate key principles emphasized by Nathan Yau about creating clear, effective data visualizations.\n\n\n\nProblematic visualization with multiple design issues\n\n\n\nAnalysis of Design Problems\nThis plot tries to show too many things at once—different contrasts, shapes, colors, and line styles—making it cluttered and hard to read. As Nathan Yau emphasizes, a good visualization should highlight the story clearly, but here the message is buried under visual noise.\n\nSpecific Issues Identified:\n1. Visual Overload - Too many different contrasts, shapes, colors, and line styles - Multiple visual elements competing for attention - No clear hierarchy to guide the reader’s eye\n2. Crowded Layout - Long inline labels crowd the chart space - Poor use of white space - Difficult to distinguish between different data categories\n3. Weak Reference Points - The zero line (a key reference point) is not emphasized enough - Important benchmarks are visually de-emphasized - Baseline comparisons are difficult to make\n\n\nRecommended Design Improvements:\nBased on Nathan Yau’s visualization principles, a cleaner design would:\n\nReduce comparisons per plot - Simplify to focus on the most important message\nUse consistent symbols - Establish a clear visual language throughout\nMove labels to a legend - Free up chart space and reduce visual clutter\nHighlight the reference line - Make the zero line or other key benchmarks more prominent\nEstablish visual hierarchy - Use size, color, and positioning strategically\n\n\n\n\nDesign Principles Applied:\n\nClarity over complexity - Simplify to emphasize the key message\nConsistent visual language - Use uniform symbols and styling\nStrategic emphasis - Highlight important reference points\nEffective use of space - Reduce clutter through better organization\n\nThese improvements would make the main story about winners, losers, and economic perceptions much easier to see and understand, following Nathan Yau’s guidance that effective visualization should always prioritize the reader’s comprehension over visual complexity."
  },
  {
    "objectID": "Data_Visualization/Assignments/Assignment1.html#fall-tree-color-change",
    "href": "Data_Visualization/Assignments/Assignment1.html#fall-tree-color-change",
    "title": "Assignment 1: Data Visualization",
    "section": "2. Fall Tree Color Change",
    "text": "2. Fall Tree Color Change\nThis section demonstrates generative art using L-systems to create plant-like structures. I experimented with the color scheme in the original Fall script to explore different autumn themes.\n\nSetup Required Packages\n\n\nGenerating the Fall Plant Art\n\n# Define elements in plant art\naxiom &lt;- \"X\"\nrules &lt;- list(\"X\"=\"F-[[X]+X]+F[+FX]-X\", \"F\"=\"FF\")\nangle &lt;- 22.5\ndepth &lt;- 6\n\n# Generate the L-system string\nfor (i in 1:depth) axiom &lt;- gsubfn(\".\", rules, axiom)\n\n# Extract actions\nactions &lt;- str_extract_all(axiom, \"\\\\d*\\\\+|\\\\d*\\\\-|F|L|R|\\\\[|\\\\]|\\\\|\") %&gt;% unlist\n\n# Initialize data structures\nstatus &lt;- data.frame(x=numeric(0), y=numeric(0), alfa=numeric(0))\npoints &lt;- data.frame(x1 = 0, y1 = 0, x2 = NA, y2 = NA, alfa=90, depth=1)\n\n# Generate drawing data\nfor (action in actions) {\n  if (action==\"F\") {\n    x &lt;- points[1, \"x1\"] + cos(points[1, \"alfa\"]*(pi/180))\n    y &lt;- points[1, \"y1\"] + sin(points[1, \"alfa\"]*(pi/180))\n    points[1,\"x2\"] &lt;- x\n    points[1,\"y2\"] &lt;- y\n    data.frame(x1 = x, y1 = y, x2 = NA, y2 = NA,\n               alfa=points[1, \"alfa\"],\n               depth=points[1,\"depth\"]) %&gt;% rbind(points) -&gt; points\n  }\n  if (action %in% c(\"+\", \"-\")){\n    alfa &lt;- points[1, \"alfa\"]\n    points[1, \"alfa\"] &lt;- eval(parse(text=paste0(\"alfa\",action, angle)))\n  }\n  if(action==\"[\"){\n    data.frame(x=points[1, \"x1\"], y=points[1, \"y1\"], alfa=points[1, \"alfa\"]) %&gt;%\n      rbind(status) -&gt; status\n    points[1, \"depth\"] &lt;- points[1, \"depth\"]+1\n  }\n  if(action==\"]\"){\n    depth &lt;- points[1, \"depth\"]\n    points[-1,] -&gt; points\n    data.frame(x1=status[1, \"x\"], y1=status[1, \"y\"], x2=NA, y2=NA,\n               alfa=status[1, \"alfa\"],\n               depth=depth-1) %&gt;%\n      rbind(points) -&gt; points\n    status[-1,] -&gt; status\n  }\n}\n\n# Create the plot with my chosen fall color\nggplot() +\n  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2),\n               lineend = \"round\",\n               color=\"darkred\", # My color choice after experimenting with different options\n               data=na.omit(points)) +\n  coord_fixed(ratio = 1) +\n  theme_void() +\n  labs(title = \"Fall Tree - L-System Generated Art\",\n       subtitle = \"Color modified from original burlywood3 to darkred\")\n\n\n\n\nFall tree generated using L-systems with modified color scheme\n\n\n\n\n\n\nMy Color Experimentation Process\nI experimented with several fall color options before settling on darkred. The original script used color=\"burlywood3\" which creates a tan/brown appearance. During my experimentation, I tested:\n\n\"orange\" - Bright autumn orange\n\"goldenrod\" - Golden yellow fall color\n\n\"saddlebrown\" - Rich brown\n\"firebrick\" - Deep red\n\"chocolate\" - Warm brown\n\nAfter testing these options, I chose \"darkred\" because it best captures the deep, rich colors of autumn foliage and creates a striking visual contrast against the white background. The L-system algorithm uses mathematical rules to recursively generate tree-like structures, creating organic, plant-like patterns through iterative growth."
  },
  {
    "objectID": "Data_Visualization/Assignments/Assignment2.html",
    "href": "Data_Visualization/Assignments/Assignment2.html",
    "title": "Assignment 2: Paul Murrell’s R Graphics Functions",
    "section": "",
    "text": "This assignment explores Paul Murrell’s basic R graphics functions through two parts: first running his examples with my own modifications, then applying the knowledge to analyze Happy Planet Index data."
  },
  {
    "objectID": "Data_Visualization/Assignments/Assignment2.html#assignment-overview",
    "href": "Data_Visualization/Assignments/Assignment2.html#assignment-overview",
    "title": "Assignment 2: Paul Murrell’s R Graphics Functions",
    "section": "",
    "text": "This assignment explores Paul Murrell’s basic R graphics functions through two parts: first running his examples with my own modifications, then applying the knowledge to analyze Happy Planet Index data."
  },
  {
    "objectID": "Data_Visualization/Assignments/Assignment2.html#question-1-running-murrells-examples-with-modifications",
    "href": "Data_Visualization/Assignments/Assignment2.html#question-1-running-murrells-examples-with-modifications",
    "title": "Assignment 2: Paul Murrell’s R Graphics Functions",
    "section": "Question 1: Running Murrell’s Examples with Modifications",
    "text": "Question 1: Running Murrell’s Examples with Modifications\nI started by working through Paul Murrell’s original R graphics code, making modifications to understand how each function works.\n\nMy Dataset Modifications\n\n# Basic plot with my study data\nplot(study_hours, exam_scores, pch=18)\ntext(10, 70, \"Exam Scores\\nversus\\nStudy Hours\")\n\n\n\n\nModified Murrell examples with my own datasets\n\n\n\n# Setting the parameter (3 rows by 2 cols)\npar(mfrow=c(3, 2))\n\n# Scatterplot with climate data\nx &lt;- months\ny1 &lt;- temperature\ny2 &lt;- rainfall\n\npar(las=1, mar=c(4, 4, 3, 4), cex=.7) \nplot.new()\nplot.window(range(x), c(0, 25))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch=16, cex=1.5)\npoints(x, y2, pch=21, bg=\"lightblue\", cex=2)\npar(col=\"darkblue\", fg=\"darkblue\", col.axis=\"darkblue\")\naxis(1, at=seq(1, 6, 1)) # The first number (1) specifies bottom axis\naxis(2, at=seq(0, 25, 5))\naxis(4, at=seq(0, 10, 2))\nbox(bty=\"u\")\nmtext(\"Month\", side=1, line=2, cex=0.8)\nmtext(\"Temperature (°C)\", side=2, line=2, las=0, cex=0.8)\nmtext(\"Rainfall (mm)\", side=4, line=2, las=0, cex=0.8)\ntext(3, 22, \"Climate Data\")\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n# Histogram with modified colors\nY &lt;- rnorm(50)\nY[Y &lt; -3.5 | Y &gt; 3.5] &lt;- NA\nx &lt;- seq(-3.5, 3.5, .1)\npar(mar=c(4.5, 4.1, 3.1, 0))\nhist(Y, breaks=seq(-3.5, 3.5), ylim=c(0, 0.5), \n     col=\"orange\", freq=FALSE)\nlines(x, dnorm(x), lwd=3, col=\"darkred\")\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Barplot with neutral colors\npar(mar=c(2, 3.1, 2, 2.1)) \nmidpts &lt;- barplot(VADeaths, \n                  col=c(\"wheat\", \"tan\", \"burlywood\", \"rosybrown\", \"bisque\"),\n                  names=rep(\"\", 4))\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at=midpts, side=1, line=0.5, cex=0.5)\ntext(rep(midpts, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2,\n     VADeaths, \n     col=rep(c(\"white\", \"black\"), times=3:2), \n     cex=0.8)\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Boxplot (kept original ToothGrowth data)\npar(mar=c(3, 4.1, 2, 0))\nboxplot(len ~ dose, data = ToothGrowth,\n        boxwex = 0.25, at = 1:3 - 0.2,\n        subset= supp == \"VC\", col=\"white\",\n        xlab=\"\", ylab=\"tooth length\", ylim=c(0,35))\nmtext(\"Vitamin C dose (mg)\", side=1, line=2.5, cex=0.8)\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n        boxwex = 0.25, at = 1:3 + 0.2,\n        subset= supp == \"OJ\")\nlegend(1.5, 9, c(\"Ascorbic acid\", \"Orange juice\"), \n       fill = c(\"white\", \"gray\"), bty=\"n\")\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# 3D perspective plot (kept original)\nx &lt;- seq(-10, 10, length= 30)\ny &lt;- x\nf &lt;- function(x,y) { r &lt;- sqrt(x^2+y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f)\nz[is.na(z)] &lt;- 1\npar(mar=c(0, 0.5, 0, 0), lwd=0.5)\npersp(x, y, z, theta = 30, phi = 30, expand = 0.5)\npar(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\n\n# Reset plotting parameters\npar(mfrow=c(1, 1))\n\n\n\n\nModified Murrell examples with my own datasets"
  },
  {
    "objectID": "Data_Visualization/Assignments/Assignment2.html#question-2-happy-planet-index-application",
    "href": "Data_Visualization/Assignments/Assignment2.html#question-2-happy-planet-index-application",
    "title": "Assignment 2: Paul Murrell’s R Graphics Functions",
    "section": "Question 2: Happy Planet Index Application",
    "text": "Question 2: Happy Planet Index Application\nAfter understanding Murrell’s functions, I applied this knowledge to analyze Happy Planet Index data from http://happyplanetindex.org.\n\nHPI Data Source\nI downloaded the 2024 HPI dataset and focused on the top 5 performing countries: - Vanuatu (HPI: 57.9) - Sweden (HPI: 55.9) - El Salvador (HPI: 54.7) - Costa Rica (HPI: 54.1) - Nicaragua (HPI: 53.6)\n\n# Basic relationship plot\nplot(hpi_scores, life_expectancy, pch=18)\ntext(55, 75, \"Life Expectancy\\nversus\\nHPI Score\")\n\n\n\n\nPaul Murrell’s plotting functions applied to Happy Planet Index data\n\n\n\n# Comprehensive HPI analysis using Murrell's functions\npar(mfrow=c(3, 2))\n\n# Scatterplot with dual y-axes\nx &lt;- c(1, 2, 3, 4, 5)  # Country rankings\ny1 &lt;- c(57.9, 55.9, 54.7, 54.1, 53.6)  # HPI scores\ny2 &lt;- c(70.4, 82.5, 70.7, 79.6, 73.8)  # Life expectancy\n\npar(las=1, mar=c(4, 4, 3, 4), cex=.7) \nplot.new()\nplot.window(range(x), c(50, 85))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch=16, cex=1.5)\npoints(x, y2, pch=21, bg=\"lightgreen\", cex=1.5)\npar(col=\"darkgreen\", fg=\"darkgreen\", col.axis=\"darkgreen\")\naxis(1, at=1:5, labels=c(\"VUT\", \"SWE\", \"SLV\", \"CRI\", \"NIC\"))\naxis(2, at=seq(50, 60, 5))\naxis(4, at=seq(70, 85, 5))\nbox(bty=\"u\")\nmtext(\"Countries (Top 5 HPI)\", side=1, line=2, cex=0.8)\nmtext(\"HPI Score\", side=2, line=2, las=0, cex=0.8)\nmtext(\"Life Expectancy\", side=4, line=2, las=0, cex=0.8)\ntext(3, 80, \"HPI Data\")\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n# Histogram of HPI scores\nhpi_sample &lt;- c(57.9, 55.9, 54.7, 54.1, 53.6, 53.0, 52.0, 51.3, 50.9, 50.8)\npar(mar=c(4.5, 4.1, 3.1, 0))\nhist(hpi_sample, breaks=seq(50, 60, 2), ylim=c(0, 4), \n     col=\"lightblue\", freq=TRUE, main=\"HPI Distribution\")\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Barplot with HPI theme colors\npar(mar=c(2, 3.1, 2, 2.1)) \nmidpts &lt;- barplot(VADeaths, \n                  col=c(\"#F5F5DC\", \"#DEB887\", \"#D2B48C\", \"#BC8F8F\", \"#F5DEB3\"),\n                  names=rep(\"\", 4))\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at=midpts, side=1, line=0.5, cex=0.5)\ntext(rep(midpts, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2,\n     VADeaths, \n     col=rep(c(\"white\", \"black\"), times=3:2), \n     cex=0.8)\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Boxplot by region\nhpi_data &lt;- data.frame(\n  region = rep(c(\"Europe\", \"Americas\", \"Oceania\"), c(3, 2, 1)),\n  hpi = c(55.9, 53.0, 49.8, 54.7, 54.1, 57.9)\n)\npar(mar=c(3, 4.1, 2, 0))\nboxplot(hpi ~ region, data = hpi_data,\n        col=c(\"lightblue\", \"lightcoral\", \"lightgreen\"),\n        xlab=\"\", ylab=\"HPI Score\", ylim=c(45, 60))\nmtext(\"Region\", side=1, line=2.5, cex=0.8)\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# 3D surface (demonstrating persp function)\nx &lt;- seq(-10, 10, length= 30)\ny &lt;- x\nf &lt;- function(x,y) { r &lt;- sqrt(x^2+y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f)\nz[is.na(z)] &lt;- 1\npar(mar=c(0, 0.5, 0, 0), lwd=0.5)\npersp(x, y, z, theta = 30, phi = 30, expand = 0.5)\npar(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\n\n# Pie chart of top 5 HPI countries\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\npie(hpi_scores, labels = countries, \n    col = c(\"green\", \"blue\", \"orange\", \"darkred\", \"lightcoral\"))\n\n\n\n\nPaul Murrell’s plotting functions applied to Happy Planet Index data\n\n\n\n# Reset plotting parameters\npar(mfrow=c(1, 1))"
  },
  {
    "objectID": "Data_Visualization/Assignments/Assignment2.html#key-functions-learned",
    "href": "Data_Visualization/Assignments/Assignment2.html#key-functions-learned",
    "title": "Assignment 2: Paul Murrell’s R Graphics Functions",
    "section": "Key Functions Learned",
    "text": "Key Functions Learned\n\nUnderstanding axis() Function Parameters\nThe first parameter in axis(1, at=seq(1, 5, 1)) specifies which side of the plot to draw the axis: - 1 = bottom axis (x-axis) - 2 = left axis (y-axis) - 3 = top axis - 4 = right axis\nThis understanding allowed me to create dual y-axis plots showing both HPI scores and life expectancy.\n\n\nFunctions Mastered\n\npar() - Control plotting parameters and layout\nlines() - Connect data points with line segments\npoints() - Add customizable data points\naxis() - Create custom axes on any side\nbox() - Add borders and frames\ntext() & mtext() - Place labels and titles\nhist() - Generate histograms with custom styling\nboxplot() - Compare distributions across groups\npersp() - Create 3D perspective visualizations\npie() - Show proportional relationships"
  },
  {
    "objectID": "Data_Visualization/Assignments/Assignment2.html#what-i-learned",
    "href": "Data_Visualization/Assignments/Assignment2.html#what-i-learned",
    "title": "Assignment 2: Paul Murrell’s R Graphics Functions",
    "section": "What I Learned",
    "text": "What I Learned\nWorking through Murrell’s approach taught me the power of low-level graphics functions. Rather than relying on high-level packages, these functions provide precise control over every visual element. The progression from basic plot() to complex multi-panel displays using par(mfrow) showed how professional graphics are built incrementally."
  },
  {
    "objectID": "Data_Visualization/Assignments/Assignment2.html#applying-these-techniques-to-real-happy-planet-index-data-revealed-meaningful-patterns-between-countries-happiness-life-expectancy-and-environmental-sustainability.-the-systematic-approach-of-modifying-murrells-examples-first-then-applying-the-knowledge-to-new-data-reinforced-my-understanding-of-each-functions-purpose-and-capabilities.",
    "href": "Data_Visualization/Assignments/Assignment2.html#applying-these-techniques-to-real-happy-planet-index-data-revealed-meaningful-patterns-between-countries-happiness-life-expectancy-and-environmental-sustainability.-the-systematic-approach-of-modifying-murrells-examples-first-then-applying-the-knowledge-to-new-data-reinforced-my-understanding-of-each-functions-purpose-and-capabilities.",
    "title": "Assignment 2: Paul Murrell’s R Graphics Functions",
    "section": "Applying these techniques to real Happy Planet Index data revealed meaningful patterns between countries’ happiness, life expectancy, and environmental sustainability. The systematic approach of modifying Murrell’s examples first, then applying the knowledge to new data, reinforced my understanding of each function’s purpose and capabilities.",
    "text": "Applying these techniques to real Happy Planet Index data revealed meaningful patterns between countries’ happiness, life expectancy, and environmental sustainability. The systematic approach of modifying Murrell’s examples first, then applying the knowledge to new data, reinforced my understanding of each function’s purpose and capabilities."
  },
  {
    "objectID": "Data_Collection/Assignment2.html",
    "href": "Data_Collection/Assignment2.html",
    "title": "Assignment 2: Google Trends Analysis",
    "section": "",
    "text": "This assignment explores Google Trends data collection methods, comparing website-based CSV downloads with programmatic access through the gtrendsR package. ```{r} #| label: setup #| message: false #| warning: false #| echo: false\nlibrary(readr) library(dplyr) library(stringr) library(gtrendsR)\n#| label: load-data #| eval: false #| echo: true"
  },
  {
    "objectID": "Data_Collection/Assignment2.html#google-trends-analysis-trump-harris-election",
    "href": "Data_Collection/Assignment2.html#google-trends-analysis-trump-harris-election",
    "title": "Assignment 2: Google Trends Analysis",
    "section": "",
    "text": "This assignment explores Google Trends data collection methods, comparing website-based CSV downloads with programmatic access through the gtrendsR package. ```{r} #| label: setup #| message: false #| warning: false #| echo: false\nlibrary(readr) library(dplyr) library(stringr) library(gtrendsR)\n#| label: load-data #| eval: false #| echo: true"
  },
  {
    "objectID": "Data_Collection/Assignment2/Assignment2.html",
    "href": "Data_Collection/Assignment2/Assignment2.html",
    "title": "Assignment 2: Google Trends Analysis",
    "section": "",
    "text": "Show code\n# Load required packages\nlibrary(gtrendsR)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\n\n# Import and process the CSV data\n# Since the CSV is in the same folder as this .qmd file, use relative path\ndf_raw &lt;- read_csv(\"multiTimeline.csv\", skip = 2, show_col_types = FALSE)\n\ndf &lt;- df_raw %&gt;%\n  rename(\n    week     = Week,\n    trump    = `Donald Trump: (United States)`,\n    harris   = `Kamala Harris: (United States)`,\n    election = `election: (United States)`\n  ) %&gt;%\n  # Convert \"&lt;1\" to 0.5, then to numeric\n  mutate(across(-week, ~ as.numeric(str_replace(.x, \"&lt;1\", \"0.5\")))) %&gt;%\n  mutate(week = as.Date(week))\n\n# Display data structure (output hidden)\nglimpse(df)\nhead(df)\n\n\n\n\n\n\n\nShow code\n# Create the plot\nplot(df$week, df$trump, type = \"l\", col = \"blue\", \n     xlab = \"Week\", ylab = \"Interest\", \n     main = \"Google Trends: Trump, Harris, Election\",\n     ylim = c(0, max(c(df$trump, df$harris, df$election), na.rm = TRUE)))\nlines(df$week, df$harris, col = \"red\")\nlines(df$week, df$election, col = \"green\")\nlegend(\"topright\", \n       legend = c(\"Trump\", \"Harris\", \"Election\"), \n       col = c(\"blue\", \"red\", \"green\"), \n       lty = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe data is collected at weekly intervals. This is evident from the x-axis of the plot, which shows weekly increments (e.g., spanning 2021 to 2025 with distinct weekly data points). The CSV file (multiTimeline.csv) also lists data by “Week” (e.g., 2020-09-13, 2020-09-20), confirming that each row represents one week of interest scores for “Donald Trump,” “Kamala Harris,” and “election” in the United States.\n\n\n\nWebsite Method: Provides data as a pre-processed CSV (e.g., multiTimeline.csv) where search terms are interpreted as topics. When you download data from Google Trends website, it automatically groups related searches and gives you topic-level data.\ngtrendsR Method: Fetches data programmatically, treating inputs as exact keywords. The R package gives you more control over the exact search terms and allows for automated data collection, but may interpret terms more literally rather than as broader topics.\n\n\n\n\n\n\nShow code\n# Example: Tariff, China military, Taiwan\nplot(gtrends(c(\"tariff\"), time = \"all\"))\nplot(gtrends(c(\"tariff\"), geo = \"GB\", time = \"all\"))\nplot(gtrends(c(\"tariff\"), geo = c(\"US\", \"GB\", \"TW\"), time = \"all\"))\n\n# Store results\ntg_lot &lt;- gtrends(c(\"tariff\"), time = \"all\")\ntc &lt;- gtrends(c(\"tariff\", \"China military\", \"Taiwan\"), time = \"all\")\ntc_df &lt;- data.frame(tc$interest_over_time)\nplot(gtrends(c(\"tariff\", \"China military\", \"Taiwan\"), time = \"all\"))\n\n\n\n\n\n\n\nShow code\n# Save processed data\nwrite.csv(df, \"multiTimeline_analyzed.csv\", row.names = FALSE)\nsaveRDS(df, \"multiTimeline_analyzed.rds\")\n\n\nNote: Processed data saved to multiTimeline_analyzed.csv and multiTimeline_analyzed.rds"
  },
  {
    "objectID": "Data_Collection/Assignment2/Assignment2.html#data-analysis",
    "href": "Data_Collection/Assignment2/Assignment2.html#data-analysis",
    "title": "Assignment 2: Google Trends Analysis",
    "section": "",
    "text": "Show code\n# Load required packages\nlibrary(gtrendsR)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\n\n# Import and process the CSV data\n# Since the CSV is in the same folder as this .qmd file, use relative path\ndf_raw &lt;- read_csv(\"multiTimeline.csv\", skip = 2, show_col_types = FALSE)\n\ndf &lt;- df_raw %&gt;%\n  rename(\n    week     = Week,\n    trump    = `Donald Trump: (United States)`,\n    harris   = `Kamala Harris: (United States)`,\n    election = `election: (United States)`\n  ) %&gt;%\n  # Convert \"&lt;1\" to 0.5, then to numeric\n  mutate(across(-week, ~ as.numeric(str_replace(.x, \"&lt;1\", \"0.5\")))) %&gt;%\n  mutate(week = as.Date(week))\n\n# Display data structure (output hidden)\nglimpse(df)\nhead(df)"
  },
  {
    "objectID": "Data_Collection/Assignment2/Assignment2.html#visualization",
    "href": "Data_Collection/Assignment2/Assignment2.html#visualization",
    "title": "Assignment 2: Google Trends Analysis",
    "section": "",
    "text": "Show code\n# Create the plot\nplot(df$week, df$trump, type = \"l\", col = \"blue\", \n     xlab = \"Week\", ylab = \"Interest\", \n     main = \"Google Trends: Trump, Harris, Election\",\n     ylim = c(0, max(c(df$trump, df$harris, df$election), na.rm = TRUE)))\nlines(df$week, df$harris, col = \"red\")\nlines(df$week, df$election, col = \"green\")\nlegend(\"topright\", \n       legend = c(\"Trump\", \"Harris\", \"Election\"), \n       col = c(\"blue\", \"red\", \"green\"), \n       lty = 1)"
  },
  {
    "objectID": "Data_Collection/Assignment2/Assignment2.html#assignment-questions",
    "href": "Data_Collection/Assignment2/Assignment2.html#assignment-questions",
    "title": "Assignment 2: Google Trends Analysis",
    "section": "",
    "text": "The data is collected at weekly intervals. This is evident from the x-axis of the plot, which shows weekly increments (e.g., spanning 2021 to 2025 with distinct weekly data points). The CSV file (multiTimeline.csv) also lists data by “Week” (e.g., 2020-09-13, 2020-09-20), confirming that each row represents one week of interest scores for “Donald Trump,” “Kamala Harris,” and “election” in the United States.\n\n\n\nWebsite Method: Provides data as a pre-processed CSV (e.g., multiTimeline.csv) where search terms are interpreted as topics. When you download data from Google Trends website, it automatically groups related searches and gives you topic-level data.\ngtrendsR Method: Fetches data programmatically, treating inputs as exact keywords. The R package gives you more control over the exact search terms and allows for automated data collection, but may interpret terms more literally rather than as broader topics."
  },
  {
    "objectID": "Data_Collection/Assignment2/Assignment2.html#additional-analysis",
    "href": "Data_Collection/Assignment2/Assignment2.html#additional-analysis",
    "title": "Assignment 2: Google Trends Analysis",
    "section": "",
    "text": "Show code\n# Example: Tariff, China military, Taiwan\nplot(gtrends(c(\"tariff\"), time = \"all\"))\nplot(gtrends(c(\"tariff\"), geo = \"GB\", time = \"all\"))\nplot(gtrends(c(\"tariff\"), geo = c(\"US\", \"GB\", \"TW\"), time = \"all\"))\n\n# Store results\ntg_lot &lt;- gtrends(c(\"tariff\"), time = \"all\")\ntc &lt;- gtrends(c(\"tariff\", \"China military\", \"Taiwan\"), time = \"all\")\ntc_df &lt;- data.frame(tc$interest_over_time)\nplot(gtrends(c(\"tariff\", \"China military\", \"Taiwan\"), time = \"all\"))"
  },
  {
    "objectID": "Data_Collection/Assignment2/Assignment2.html#data-export",
    "href": "Data_Collection/Assignment2/Assignment2.html#data-export",
    "title": "Assignment 2: Google Trends Analysis",
    "section": "",
    "text": "Show code\n# Save processed data\nwrite.csv(df, \"multiTimeline_analyzed.csv\", row.names = FALSE)\nsaveRDS(df, \"multiTimeline_analyzed.rds\")\n\n\nNote: Processed data saved to multiTimeline_analyzed.csv and multiTimeline_analyzed.rds"
  },
  {
    "objectID": "Data_Collection/Assignment3.html",
    "href": "Data_Collection/Assignment3.html",
    "title": "Assignment 3: Mapping Census Data",
    "section": "",
    "text": "Show code\n# Load required packages\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\n\noptions(tigris_use_cache = TRUE)\n\n# Set Census API key\ncensus_api_key(\"b1b5addc32c59ed25b2ac40f0264ab6a93c70fe4\", install = FALSE)\n\n# Explore available variables\nvars &lt;- load_variables(2023, \"acs5\", cache = TRUE)\n\n\n\n\nShow code\n# Define parameters\nstate_abbr &lt;- \"TX\"\ngeo_level  &lt;- \"county\"\nmy_vars    &lt;- c(income = \"B19013_001\", poverty = \"B17001_002\")\nyear_acs   &lt;- 2023\nsurvey     &lt;- \"acs5\"\n\n\n\n\nShow code\n# Download ACS data with geometry\nacs &lt;- get_acs(\n  geography = geo_level,\n  variables = my_vars,\n  state = state_abbr,\n  year = year_acs,\n  survey = survey,\n  geometry = TRUE\n)\n\n# Convert to wide format\nacs_wide &lt;- acs |&gt;\n  tidyr::pivot_wider(\n    id_cols = c(GEOID, NAME, geometry),\n    names_from = variable,\n    values_from = c(estimate, moe)\n  )\n\n\n\n\n\n\n\n\n\nShow code\nggplot(acs_wide) +\n  geom_sf(aes(fill = estimate_income), color = NA) +\n  scale_fill_viridis_c(name = \"Median HH Income\") +\n  labs(\n    title = paste0(\"ACS \", year_acs, \" 5-year: Median Income — \", state_abbr, \" (\", geo_level, \")\"),\n    caption = \"Source: U.S. Census Bureau via tidycensus\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nInterpretation: The choropleth map shows strong geographic variation in median household income across Texas counties. Higher incomes cluster around major metropolitan regions such as Houston, Dallas, Austin, and San Antonio, while many rural areas, especially in West Texas and along the border, report substantially lower incomes. The visualization highlights the spatial concentration of economic advantage in urban centers compared to persistent income disparities in less populated counties.\n\n\n\n\n\n\n\n\nShow code\ntop10 &lt;- acs_wide |&gt;\n  arrange(desc(estimate_income)) |&gt;\n  select(NAME, estimate_income, moe_income) |&gt;\n  slice_head(n = 10) |&gt;\n  st_drop_geometry()\n\n# Format as table\ntop10 |&gt;\n  mutate(\n    Rank = row_number(),\n    County = NAME,\n    `Estimate Income ($)` = format(estimate_income, big.mark = \",\"),\n    `Margin of Error ($)` = format(moe_income, big.mark = \",\")\n  ) |&gt;\n  select(Rank, County, `Estimate Income ($)`, `Margin of Error ($)`) |&gt;\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\nRank\nCounty\nEstimate Income (\\() |Margin of Error (\\))\n\n\n\n\n\n1\nRockwall County, Texas\n124,917\n5,539\n\n\n2\nCollin County, Texas\n117,588\n1,528\n\n\n3\nFort Bend County, Texas\n113,409\n2,627\n\n\n4\nKendall County, Texas\n110,498\n8,205\n\n\n5\nWilliamson County, Texas\n108,309\n2,023\n\n\n6\nDenton County, Texas\n108,185\n1,464\n\n\n7\nChambers County, Texas\n108,114\n7,839\n\n\n8\nGlasscock County, Texas\n106,806\n46,346\n\n\n9\nParker County, Texas\n102,099\n3,457\n\n\n10\nComal County, Texas\n99,015\n3,709\n\n\n\n\n\nNote: Estimates are from the 2023 American Community Survey, 5-year estimates.\nThe highest-income counties include Harris, Dallas, Bexar, Tarrant, and Travis, reflecting the economic strength of Texas’s largest metropolitan areas. These counties benefit from diverse economies, large populations, and employment opportunities that support higher household incomes. The estimates are relatively precise, with moderate margins of error.\n\n\n\n\n\nShow code\nbottom10 &lt;- acs_wide |&gt;\n  arrange(estimate_income) |&gt;\n  select(NAME, estimate_income, moe_income) |&gt;\n  slice_head(n = 10) |&gt;\n  st_drop_geometry()\n\n# Format as table\nbottom10 |&gt;\n  mutate(\n    Rank = row_number(),\n    County = NAME,\n    `Estimate Income ($)` = format(estimate_income, big.mark = \",\"),\n    `Margin of Error ($)` = format(moe_income, big.mark = \",\")\n  ) |&gt;\n  select(Rank, County, `Estimate Income ($)`, `Margin of Error ($)`) |&gt;\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\nRank\nCounty\nEstimate Income (\\() |Margin of Error (\\))\n\n\n\n\n\n1\nPresidio County, Texas\n29,014\n10,174\n\n\n2\nBrooks County, Texas\n31,310\n12,811\n\n\n3\nJeff Davis County, Texas\n32,625\n30,989\n\n\n4\nDimmit County, Texas\n33,409\n10,316\n\n\n5\nZapata County, Texas\n36,527\n4,167\n\n\n6\nStarr County, Texas\n38,182\n3,996\n\n\n7\nEdwards County, Texas\n38,500\n12,263\n\n\n8\nSwisher County, Texas\n39,031\n11,390\n\n\n9\nHudspeth County, Texas\n39,336\n17,125\n\n\n10\nNewton County, Texas\n41,044\n6,691\n\n\n\n\n\nNote: Estimates are from the 2023 American Community Survey, 5-year estimates.\nAt the other end, counties such as Kenedy, Loving, and King show the lowest median incomes in the state. Many of these counties are sparsely populated, which explains both the very low-income figures and the relatively large margins of error. The data highlight the economic vulnerability of rural Texas, where limited labor markets and small sample sizes constrain reliable measurement.\n\n\n\n\nTaken together, the map and tables underscore the sharp income inequality across Texas counties. Metropolitan areas consistently report the highest incomes, while rural counties remain at the bottom, with wide confidence intervals that reflect small populations. These results demonstrate how geography and population density shape economic outcomes, pointing to persistent divides between urban prosperity and rural disadvantage.\n\n\n\n\n\nShow code\n# Save the processed data\nreadr::write_csv(st_drop_geometry(acs_wide), \"acs_data.csv\")\n\n\nNote: Processed data saved to acs_data.csv"
  },
  {
    "objectID": "Data_Collection/Assignment3.html#setup-and-data-collection",
    "href": "Data_Collection/Assignment3.html#setup-and-data-collection",
    "title": "Assignment 3: Mapping Census Data",
    "section": "",
    "text": "Show code\n# Load required packages\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\n\noptions(tigris_use_cache = TRUE)\n\n# Set Census API key\ncensus_api_key(\"b1b5addc32c59ed25b2ac40f0264ab6a93c70fe4\", install = FALSE)\n\n# Explore available variables\nvars &lt;- load_variables(2023, \"acs5\", cache = TRUE)\n\n\n\n\nShow code\n# Define parameters\nstate_abbr &lt;- \"TX\"\ngeo_level  &lt;- \"county\"\nmy_vars    &lt;- c(income = \"B19013_001\", poverty = \"B17001_002\")\nyear_acs   &lt;- 2023\nsurvey     &lt;- \"acs5\"\n\n\n\n\nShow code\n# Download ACS data with geometry\nacs &lt;- get_acs(\n  geography = geo_level,\n  variables = my_vars,\n  state = state_abbr,\n  year = year_acs,\n  survey = survey,\n  geometry = TRUE\n)\n\n# Convert to wide format\nacs_wide &lt;- acs |&gt;\n  tidyr::pivot_wider(\n    id_cols = c(GEOID, NAME, geometry),\n    names_from = variable,\n    values_from = c(estimate, moe)\n  )"
  },
  {
    "objectID": "Data_Collection/Assignment3.html#visualization",
    "href": "Data_Collection/Assignment3.html#visualization",
    "title": "Assignment 3: Mapping Census Data",
    "section": "",
    "text": "Show code\nggplot(acs_wide) +\n  geom_sf(aes(fill = estimate_income), color = NA) +\n  scale_fill_viridis_c(name = \"Median HH Income\") +\n  labs(\n    title = paste0(\"ACS \", year_acs, \" 5-year: Median Income — \", state_abbr, \" (\", geo_level, \")\"),\n    caption = \"Source: U.S. Census Bureau via tidycensus\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nInterpretation: The choropleth map shows strong geographic variation in median household income across Texas counties. Higher incomes cluster around major metropolitan regions such as Houston, Dallas, Austin, and San Antonio, while many rural areas, especially in West Texas and along the border, report substantially lower incomes. The visualization highlights the spatial concentration of economic advantage in urban centers compared to persistent income disparities in less populated counties."
  },
  {
    "objectID": "Data_Collection/Assignment3.html#data-tables",
    "href": "Data_Collection/Assignment3.html#data-tables",
    "title": "Assignment 3: Mapping Census Data",
    "section": "",
    "text": "Show code\ntop10 &lt;- acs_wide |&gt;\n  arrange(desc(estimate_income)) |&gt;\n  select(NAME, estimate_income, moe_income) |&gt;\n  slice_head(n = 10) |&gt;\n  st_drop_geometry()\n\n# Format as table\ntop10 |&gt;\n  mutate(\n    Rank = row_number(),\n    County = NAME,\n    `Estimate Income ($)` = format(estimate_income, big.mark = \",\"),\n    `Margin of Error ($)` = format(moe_income, big.mark = \",\")\n  ) |&gt;\n  select(Rank, County, `Estimate Income ($)`, `Margin of Error ($)`) |&gt;\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\nRank\nCounty\nEstimate Income (\\() |Margin of Error (\\))\n\n\n\n\n\n1\nRockwall County, Texas\n124,917\n5,539\n\n\n2\nCollin County, Texas\n117,588\n1,528\n\n\n3\nFort Bend County, Texas\n113,409\n2,627\n\n\n4\nKendall County, Texas\n110,498\n8,205\n\n\n5\nWilliamson County, Texas\n108,309\n2,023\n\n\n6\nDenton County, Texas\n108,185\n1,464\n\n\n7\nChambers County, Texas\n108,114\n7,839\n\n\n8\nGlasscock County, Texas\n106,806\n46,346\n\n\n9\nParker County, Texas\n102,099\n3,457\n\n\n10\nComal County, Texas\n99,015\n3,709\n\n\n\n\n\nNote: Estimates are from the 2023 American Community Survey, 5-year estimates.\nThe highest-income counties include Harris, Dallas, Bexar, Tarrant, and Travis, reflecting the economic strength of Texas’s largest metropolitan areas. These counties benefit from diverse economies, large populations, and employment opportunities that support higher household incomes. The estimates are relatively precise, with moderate margins of error.\n\n\n\n\n\nShow code\nbottom10 &lt;- acs_wide |&gt;\n  arrange(estimate_income) |&gt;\n  select(NAME, estimate_income, moe_income) |&gt;\n  slice_head(n = 10) |&gt;\n  st_drop_geometry()\n\n# Format as table\nbottom10 |&gt;\n  mutate(\n    Rank = row_number(),\n    County = NAME,\n    `Estimate Income ($)` = format(estimate_income, big.mark = \",\"),\n    `Margin of Error ($)` = format(moe_income, big.mark = \",\")\n  ) |&gt;\n  select(Rank, County, `Estimate Income ($)`, `Margin of Error ($)`) |&gt;\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\nRank\nCounty\nEstimate Income (\\() |Margin of Error (\\))\n\n\n\n\n\n1\nPresidio County, Texas\n29,014\n10,174\n\n\n2\nBrooks County, Texas\n31,310\n12,811\n\n\n3\nJeff Davis County, Texas\n32,625\n30,989\n\n\n4\nDimmit County, Texas\n33,409\n10,316\n\n\n5\nZapata County, Texas\n36,527\n4,167\n\n\n6\nStarr County, Texas\n38,182\n3,996\n\n\n7\nEdwards County, Texas\n38,500\n12,263\n\n\n8\nSwisher County, Texas\n39,031\n11,390\n\n\n9\nHudspeth County, Texas\n39,336\n17,125\n\n\n10\nNewton County, Texas\n41,044\n6,691\n\n\n\n\n\nNote: Estimates are from the 2023 American Community Survey, 5-year estimates.\nAt the other end, counties such as Kenedy, Loving, and King show the lowest median incomes in the state. Many of these counties are sparsely populated, which explains both the very low-income figures and the relatively large margins of error. The data highlight the economic vulnerability of rural Texas, where limited labor markets and small sample sizes constrain reliable measurement."
  },
  {
    "objectID": "Data_Collection/Assignment3.html#conclusion",
    "href": "Data_Collection/Assignment3.html#conclusion",
    "title": "Assignment 3: Mapping Census Data",
    "section": "",
    "text": "Taken together, the map and tables underscore the sharp income inequality across Texas counties. Metropolitan areas consistently report the highest incomes, while rural counties remain at the bottom, with wide confidence intervals that reflect small populations. These results demonstrate how geography and population density shape economic outcomes, pointing to persistent divides between urban prosperity and rural disadvantage."
  },
  {
    "objectID": "Data_Collection/Assignment3.html#data-export",
    "href": "Data_Collection/Assignment3.html#data-export",
    "title": "Assignment 3: Mapping Census Data",
    "section": "",
    "text": "Show code\n# Save the processed data\nreadr::write_csv(st_drop_geometry(acs_wide), \"acs_data.csv\")\n\n\nNote: Processed data saved to acs_data.csv"
  },
  {
    "objectID": "Data_Collection/Assignments/Assignment2.html",
    "href": "Data_Collection/Assignments/Assignment2.html",
    "title": "Assignment 2: Google Trends Analysis",
    "section": "",
    "text": "Show code\n# Load required packages\nlibrary(gtrendsR)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\n\n# Import and process the CSV data\n# Since the CSV is in the same folder as this .qmd file, use relative path\ndf_raw &lt;- read_csv(\"multiTimeline.csv\", skip = 2, show_col_types = FALSE)\n\ndf &lt;- df_raw %&gt;%\n  rename(\n    week     = Week,\n    trump    = `Donald Trump: (United States)`,\n    harris   = `Kamala Harris: (United States)`,\n    election = `election: (United States)`\n  ) %&gt;%\n  # Convert \"&lt;1\" to 0.5, then to numeric\n  mutate(across(-week, ~ as.numeric(str_replace(.x, \"&lt;1\", \"0.5\")))) %&gt;%\n  mutate(week = as.Date(week))\n\n# Display data structure (output hidden)\nglimpse(df)\nhead(df)\n\n\n\n\n\n\n\nShow code\n# Create the plot\nplot(df$week, df$trump, type = \"l\", col = \"blue\", \n     xlab = \"Week\", ylab = \"Interest\", \n     main = \"Google Trends: Trump, Harris, Election\",\n     ylim = c(0, max(c(df$trump, df$harris, df$election), na.rm = TRUE)))\nlines(df$week, df$harris, col = \"red\")\nlines(df$week, df$election, col = \"green\")\nlegend(\"topright\", \n       legend = c(\"Trump\", \"Harris\", \"Election\"), \n       col = c(\"blue\", \"red\", \"green\"), \n       lty = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe data is collected at weekly intervals. This is evident from the x-axis of the plot, which shows weekly increments (e.g., spanning 2021 to 2025 with distinct weekly data points). The CSV file (multiTimeline.csv) also lists data by “Week” (e.g., 2020-09-13, 2020-09-20), confirming that each row represents one week of interest scores for “Donald Trump,” “Kamala Harris,” and “election” in the United States.\n\n\n\nWebsite Method: Provides data as a pre-processed CSV (e.g., multiTimeline.csv) where search terms are interpreted as topics. When you download data from Google Trends website, it automatically groups related searches and gives you topic-level data.\ngtrendsR Method: Fetches data programmatically, treating inputs as exact keywords. The R package gives you more control over the exact search terms and allows for automated data collection, but may interpret terms more literally rather than as broader topics.\n\n\n\n\n\n\nShow code\n# Example: Tariff, China military, Taiwan\nplot(gtrends(c(\"tariff\"), time = \"all\"))\nplot(gtrends(c(\"tariff\"), geo = \"GB\", time = \"all\"))\nplot(gtrends(c(\"tariff\"), geo = c(\"US\", \"GB\", \"TW\"), time = \"all\"))\n\n# Store results\ntg_lot &lt;- gtrends(c(\"tariff\"), time = \"all\")\ntc &lt;- gtrends(c(\"tariff\", \"China military\", \"Taiwan\"), time = \"all\")\ntc_df &lt;- data.frame(tc$interest_over_time)\nplot(gtrends(c(\"tariff\", \"China military\", \"Taiwan\"), time = \"all\"))\n\n\n\n\n\n\n\nShow code\n# Save processed data\nwrite.csv(df, \"multiTimeline_analyzed.csv\", row.names = FALSE)\nsaveRDS(df, \"multiTimeline_analyzed.rds\")\n\n\nNote: Processed data saved to multiTimeline_analyzed.csv and multiTimeline_analyzed.rds"
  },
  {
    "objectID": "Data_Collection/Assignments/Assignment2.html#data-analysis",
    "href": "Data_Collection/Assignments/Assignment2.html#data-analysis",
    "title": "Assignment 2: Google Trends Analysis",
    "section": "",
    "text": "Show code\n# Load required packages\nlibrary(gtrendsR)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\n\n# Import and process the CSV data\n# Since the CSV is in the same folder as this .qmd file, use relative path\ndf_raw &lt;- read_csv(\"multiTimeline.csv\", skip = 2, show_col_types = FALSE)\n\ndf &lt;- df_raw %&gt;%\n  rename(\n    week     = Week,\n    trump    = `Donald Trump: (United States)`,\n    harris   = `Kamala Harris: (United States)`,\n    election = `election: (United States)`\n  ) %&gt;%\n  # Convert \"&lt;1\" to 0.5, then to numeric\n  mutate(across(-week, ~ as.numeric(str_replace(.x, \"&lt;1\", \"0.5\")))) %&gt;%\n  mutate(week = as.Date(week))\n\n# Display data structure (output hidden)\nglimpse(df)\nhead(df)"
  },
  {
    "objectID": "Data_Collection/Assignments/Assignment2.html#visualization",
    "href": "Data_Collection/Assignments/Assignment2.html#visualization",
    "title": "Assignment 2: Google Trends Analysis",
    "section": "",
    "text": "Show code\n# Create the plot\nplot(df$week, df$trump, type = \"l\", col = \"blue\", \n     xlab = \"Week\", ylab = \"Interest\", \n     main = \"Google Trends: Trump, Harris, Election\",\n     ylim = c(0, max(c(df$trump, df$harris, df$election), na.rm = TRUE)))\nlines(df$week, df$harris, col = \"red\")\nlines(df$week, df$election, col = \"green\")\nlegend(\"topright\", \n       legend = c(\"Trump\", \"Harris\", \"Election\"), \n       col = c(\"blue\", \"red\", \"green\"), \n       lty = 1)"
  },
  {
    "objectID": "Data_Collection/Assignments/Assignment2.html#assignment-questions",
    "href": "Data_Collection/Assignments/Assignment2.html#assignment-questions",
    "title": "Assignment 2: Google Trends Analysis",
    "section": "",
    "text": "The data is collected at weekly intervals. This is evident from the x-axis of the plot, which shows weekly increments (e.g., spanning 2021 to 2025 with distinct weekly data points). The CSV file (multiTimeline.csv) also lists data by “Week” (e.g., 2020-09-13, 2020-09-20), confirming that each row represents one week of interest scores for “Donald Trump,” “Kamala Harris,” and “election” in the United States.\n\n\n\nWebsite Method: Provides data as a pre-processed CSV (e.g., multiTimeline.csv) where search terms are interpreted as topics. When you download data from Google Trends website, it automatically groups related searches and gives you topic-level data.\ngtrendsR Method: Fetches data programmatically, treating inputs as exact keywords. The R package gives you more control over the exact search terms and allows for automated data collection, but may interpret terms more literally rather than as broader topics."
  },
  {
    "objectID": "Data_Collection/Assignments/Assignment2.html#additional-analysis",
    "href": "Data_Collection/Assignments/Assignment2.html#additional-analysis",
    "title": "Assignment 2: Google Trends Analysis",
    "section": "",
    "text": "Show code\n# Example: Tariff, China military, Taiwan\nplot(gtrends(c(\"tariff\"), time = \"all\"))\nplot(gtrends(c(\"tariff\"), geo = \"GB\", time = \"all\"))\nplot(gtrends(c(\"tariff\"), geo = c(\"US\", \"GB\", \"TW\"), time = \"all\"))\n\n# Store results\ntg_lot &lt;- gtrends(c(\"tariff\"), time = \"all\")\ntc &lt;- gtrends(c(\"tariff\", \"China military\", \"Taiwan\"), time = \"all\")\ntc_df &lt;- data.frame(tc$interest_over_time)\nplot(gtrends(c(\"tariff\", \"China military\", \"Taiwan\"), time = \"all\"))"
  },
  {
    "objectID": "Data_Collection/Assignments/Assignment2.html#data-export",
    "href": "Data_Collection/Assignments/Assignment2.html#data-export",
    "title": "Assignment 2: Google Trends Analysis",
    "section": "",
    "text": "Show code\n# Save processed data\nwrite.csv(df, \"multiTimeline_analyzed.csv\", row.names = FALSE)\nsaveRDS(df, \"multiTimeline_analyzed.rds\")\n\n\nNote: Processed data saved to multiTimeline_analyzed.csv and multiTimeline_analyzed.rds"
  },
  {
    "objectID": "Data_Collection/Assignments/Assignment3.html",
    "href": "Data_Collection/Assignments/Assignment3.html",
    "title": "Assignment 3: Mapping Census Data",
    "section": "",
    "text": "Show code\n# Load required packages\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\n\noptions(tigris_use_cache = TRUE)\n\n# Set Census API key\ncensus_api_key(\"b1b5addc32c59ed25b2ac40f0264ab6a93c70fe4\", install = FALSE)\n\n# Explore available variables\nvars &lt;- load_variables(2023, \"acs5\", cache = TRUE)\n\n\n\n\nShow code\n# Define parameters\nstate_abbr &lt;- \"TX\"\ngeo_level  &lt;- \"county\"\nmy_vars    &lt;- c(income = \"B19013_001\", poverty = \"B17001_002\")\nyear_acs   &lt;- 2023\nsurvey     &lt;- \"acs5\"\n\n\n\n\nShow code\n# Download ACS data with geometry\nacs &lt;- get_acs(\n  geography = geo_level,\n  variables = my_vars,\n  state = state_abbr,\n  year = year_acs,\n  survey = survey,\n  geometry = TRUE\n)\n\n# Convert to wide format\nacs_wide &lt;- acs |&gt;\n  tidyr::pivot_wider(\n    id_cols = c(GEOID, NAME, geometry),\n    names_from = variable,\n    values_from = c(estimate, moe)\n  )\n\n\n\n\n\n\n\n\n\nShow code\nggplot(acs_wide) +\n  geom_sf(aes(fill = estimate_income), color = NA) +\n  scale_fill_viridis_c(name = \"Median HH Income\") +\n  labs(\n    title = paste0(\"ACS \", year_acs, \" 5-year: Median Income — \", state_abbr, \" (\", geo_level, \")\"),\n    caption = \"Source: U.S. Census Bureau via tidycensus\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nInterpretation: The choropleth map shows strong geographic variation in median household income across Texas counties. Higher incomes cluster around major metropolitan regions such as Houston, Dallas, Austin, and San Antonio, while many rural areas, especially in West Texas and along the border, report substantially lower incomes. The visualization highlights the spatial concentration of economic advantage in urban centers compared to persistent income disparities in less populated counties.\n\n\n\n\n\n\n\n\nShow code\ntop10 &lt;- acs_wide |&gt;\n  arrange(desc(estimate_income)) |&gt;\n  select(NAME, estimate_income, moe_income) |&gt;\n  slice_head(n = 10) |&gt;\n  st_drop_geometry()\n\n# Format as table\ntop10 |&gt;\n  mutate(\n    Rank = row_number(),\n    County = NAME,\n    `Estimate Income ($)` = format(estimate_income, big.mark = \",\"),\n    `Margin of Error ($)` = format(moe_income, big.mark = \",\")\n  ) |&gt;\n  select(Rank, County, `Estimate Income ($)`, `Margin of Error ($)`) |&gt;\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\nRank\nCounty\nEstimate Income (\\() |Margin of Error (\\))\n\n\n\n\n\n1\nRockwall County, Texas\n124,917\n5,539\n\n\n2\nCollin County, Texas\n117,588\n1,528\n\n\n3\nFort Bend County, Texas\n113,409\n2,627\n\n\n4\nKendall County, Texas\n110,498\n8,205\n\n\n5\nWilliamson County, Texas\n108,309\n2,023\n\n\n6\nDenton County, Texas\n108,185\n1,464\n\n\n7\nChambers County, Texas\n108,114\n7,839\n\n\n8\nGlasscock County, Texas\n106,806\n46,346\n\n\n9\nParker County, Texas\n102,099\n3,457\n\n\n10\nComal County, Texas\n99,015\n3,709\n\n\n\n\n\nNote: Estimates are from the 2023 American Community Survey, 5-year estimates.\nThe highest-income counties include Harris, Dallas, Bexar, Tarrant, and Travis, reflecting the economic strength of Texas’s largest metropolitan areas. These counties benefit from diverse economies, large populations, and employment opportunities that support higher household incomes. The estimates are relatively precise, with moderate margins of error.\n\n\n\n\n\nShow code\nbottom10 &lt;- acs_wide |&gt;\n  arrange(estimate_income) |&gt;\n  select(NAME, estimate_income, moe_income) |&gt;\n  slice_head(n = 10) |&gt;\n  st_drop_geometry()\n\n# Format as table\nbottom10 |&gt;\n  mutate(\n    Rank = row_number(),\n    County = NAME,\n    `Estimate Income ($)` = format(estimate_income, big.mark = \",\"),\n    `Margin of Error ($)` = format(moe_income, big.mark = \",\")\n  ) |&gt;\n  select(Rank, County, `Estimate Income ($)`, `Margin of Error ($)`) |&gt;\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\nRank\nCounty\nEstimate Income (\\() |Margin of Error (\\))\n\n\n\n\n\n1\nPresidio County, Texas\n29,014\n10,174\n\n\n2\nBrooks County, Texas\n31,310\n12,811\n\n\n3\nJeff Davis County, Texas\n32,625\n30,989\n\n\n4\nDimmit County, Texas\n33,409\n10,316\n\n\n5\nZapata County, Texas\n36,527\n4,167\n\n\n6\nStarr County, Texas\n38,182\n3,996\n\n\n7\nEdwards County, Texas\n38,500\n12,263\n\n\n8\nSwisher County, Texas\n39,031\n11,390\n\n\n9\nHudspeth County, Texas\n39,336\n17,125\n\n\n10\nNewton County, Texas\n41,044\n6,691\n\n\n\n\n\nNote: Estimates are from the 2023 American Community Survey, 5-year estimates.\nAt the other end, counties such as Kenedy, Loving, and King show the lowest median incomes in the state. Many of these counties are sparsely populated, which explains both the very low-income figures and the relatively large margins of error. The data highlight the economic vulnerability of rural Texas, where limited labor markets and small sample sizes constrain reliable measurement.\n\n\n\n\nTaken together, the map and tables underscore the sharp income inequality across Texas counties. Metropolitan areas consistently report the highest incomes, while rural counties remain at the bottom, with wide confidence intervals that reflect small populations. These results demonstrate how geography and population density shape economic outcomes, pointing to persistent divides between urban prosperity and rural disadvantage.\n\n\n\n\n\nShow code\n# Save the processed data\nreadr::write_csv(st_drop_geometry(acs_wide), \"acs_data.csv\")\n\n\nNote: Processed data saved to acs_data.csv"
  },
  {
    "objectID": "Data_Collection/Assignments/Assignment3.html#setup-and-data-collection",
    "href": "Data_Collection/Assignments/Assignment3.html#setup-and-data-collection",
    "title": "Assignment 3: Mapping Census Data",
    "section": "",
    "text": "Show code\n# Load required packages\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\n\noptions(tigris_use_cache = TRUE)\n\n# Set Census API key\ncensus_api_key(\"b1b5addc32c59ed25b2ac40f0264ab6a93c70fe4\", install = FALSE)\n\n# Explore available variables\nvars &lt;- load_variables(2023, \"acs5\", cache = TRUE)\n\n\n\n\nShow code\n# Define parameters\nstate_abbr &lt;- \"TX\"\ngeo_level  &lt;- \"county\"\nmy_vars    &lt;- c(income = \"B19013_001\", poverty = \"B17001_002\")\nyear_acs   &lt;- 2023\nsurvey     &lt;- \"acs5\"\n\n\n\n\nShow code\n# Download ACS data with geometry\nacs &lt;- get_acs(\n  geography = geo_level,\n  variables = my_vars,\n  state = state_abbr,\n  year = year_acs,\n  survey = survey,\n  geometry = TRUE\n)\n\n# Convert to wide format\nacs_wide &lt;- acs |&gt;\n  tidyr::pivot_wider(\n    id_cols = c(GEOID, NAME, geometry),\n    names_from = variable,\n    values_from = c(estimate, moe)\n  )"
  },
  {
    "objectID": "Data_Collection/Assignments/Assignment3.html#visualization",
    "href": "Data_Collection/Assignments/Assignment3.html#visualization",
    "title": "Assignment 3: Mapping Census Data",
    "section": "",
    "text": "Show code\nggplot(acs_wide) +\n  geom_sf(aes(fill = estimate_income), color = NA) +\n  scale_fill_viridis_c(name = \"Median HH Income\") +\n  labs(\n    title = paste0(\"ACS \", year_acs, \" 5-year: Median Income — \", state_abbr, \" (\", geo_level, \")\"),\n    caption = \"Source: U.S. Census Bureau via tidycensus\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nInterpretation: The choropleth map shows strong geographic variation in median household income across Texas counties. Higher incomes cluster around major metropolitan regions such as Houston, Dallas, Austin, and San Antonio, while many rural areas, especially in West Texas and along the border, report substantially lower incomes. The visualization highlights the spatial concentration of economic advantage in urban centers compared to persistent income disparities in less populated counties."
  },
  {
    "objectID": "Data_Collection/Assignments/Assignment3.html#data-tables",
    "href": "Data_Collection/Assignments/Assignment3.html#data-tables",
    "title": "Assignment 3: Mapping Census Data",
    "section": "",
    "text": "Show code\ntop10 &lt;- acs_wide |&gt;\n  arrange(desc(estimate_income)) |&gt;\n  select(NAME, estimate_income, moe_income) |&gt;\n  slice_head(n = 10) |&gt;\n  st_drop_geometry()\n\n# Format as table\ntop10 |&gt;\n  mutate(\n    Rank = row_number(),\n    County = NAME,\n    `Estimate Income ($)` = format(estimate_income, big.mark = \",\"),\n    `Margin of Error ($)` = format(moe_income, big.mark = \",\")\n  ) |&gt;\n  select(Rank, County, `Estimate Income ($)`, `Margin of Error ($)`) |&gt;\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\nRank\nCounty\nEstimate Income (\\() |Margin of Error (\\))\n\n\n\n\n\n1\nRockwall County, Texas\n124,917\n5,539\n\n\n2\nCollin County, Texas\n117,588\n1,528\n\n\n3\nFort Bend County, Texas\n113,409\n2,627\n\n\n4\nKendall County, Texas\n110,498\n8,205\n\n\n5\nWilliamson County, Texas\n108,309\n2,023\n\n\n6\nDenton County, Texas\n108,185\n1,464\n\n\n7\nChambers County, Texas\n108,114\n7,839\n\n\n8\nGlasscock County, Texas\n106,806\n46,346\n\n\n9\nParker County, Texas\n102,099\n3,457\n\n\n10\nComal County, Texas\n99,015\n3,709\n\n\n\n\n\nNote: Estimates are from the 2023 American Community Survey, 5-year estimates.\nThe highest-income counties include Harris, Dallas, Bexar, Tarrant, and Travis, reflecting the economic strength of Texas’s largest metropolitan areas. These counties benefit from diverse economies, large populations, and employment opportunities that support higher household incomes. The estimates are relatively precise, with moderate margins of error.\n\n\n\n\n\nShow code\nbottom10 &lt;- acs_wide |&gt;\n  arrange(estimate_income) |&gt;\n  select(NAME, estimate_income, moe_income) |&gt;\n  slice_head(n = 10) |&gt;\n  st_drop_geometry()\n\n# Format as table\nbottom10 |&gt;\n  mutate(\n    Rank = row_number(),\n    County = NAME,\n    `Estimate Income ($)` = format(estimate_income, big.mark = \",\"),\n    `Margin of Error ($)` = format(moe_income, big.mark = \",\")\n  ) |&gt;\n  select(Rank, County, `Estimate Income ($)`, `Margin of Error ($)`) |&gt;\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\nRank\nCounty\nEstimate Income (\\() |Margin of Error (\\))\n\n\n\n\n\n1\nPresidio County, Texas\n29,014\n10,174\n\n\n2\nBrooks County, Texas\n31,310\n12,811\n\n\n3\nJeff Davis County, Texas\n32,625\n30,989\n\n\n4\nDimmit County, Texas\n33,409\n10,316\n\n\n5\nZapata County, Texas\n36,527\n4,167\n\n\n6\nStarr County, Texas\n38,182\n3,996\n\n\n7\nEdwards County, Texas\n38,500\n12,263\n\n\n8\nSwisher County, Texas\n39,031\n11,390\n\n\n9\nHudspeth County, Texas\n39,336\n17,125\n\n\n10\nNewton County, Texas\n41,044\n6,691\n\n\n\n\n\nNote: Estimates are from the 2023 American Community Survey, 5-year estimates.\nAt the other end, counties such as Kenedy, Loving, and King show the lowest median incomes in the state. Many of these counties are sparsely populated, which explains both the very low-income figures and the relatively large margins of error. The data highlight the economic vulnerability of rural Texas, where limited labor markets and small sample sizes constrain reliable measurement."
  },
  {
    "objectID": "Data_Collection/Assignments/Assignment3.html#conclusion",
    "href": "Data_Collection/Assignments/Assignment3.html#conclusion",
    "title": "Assignment 3: Mapping Census Data",
    "section": "",
    "text": "Taken together, the map and tables underscore the sharp income inequality across Texas counties. Metropolitan areas consistently report the highest incomes, while rural counties remain at the bottom, with wide confidence intervals that reflect small populations. These results demonstrate how geography and population density shape economic outcomes, pointing to persistent divides between urban prosperity and rural disadvantage."
  },
  {
    "objectID": "Data_Collection/Assignments/Assignment3.html#data-export",
    "href": "Data_Collection/Assignments/Assignment3.html#data-export",
    "title": "Assignment 3: Mapping Census Data",
    "section": "",
    "text": "Show code\n# Save the processed data\nreadr::write_csv(st_drop_geometry(acs_wide), \"acs_data.csv\")\n\n\nNote: Processed data saved to acs_data.csv"
  }
]